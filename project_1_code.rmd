---
title: "STAT 27850 Project 1 Code"
author: "Tony Zong, Louisa Lyu, Stanley Zhu"
date: '2023-01-28'
geometry: margin=2cm
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.align = 'center')
library(tidyverse)
rm(list=ls())
```

# Load Data and Data Cleaing

Might need some further cleaning

```{r}
# Temperature Data
temp_data <- 
  read.table("data/temperature-data.txt", 
           skip = 1, 
           col.names = c('time1', 'time2', 'daily_max', 'daily_min')) %>% 
  mutate(year = as.numeric(substring(time2, 1, 4))) %>% 
  mutate(month = as.numeric(substring(time2, 6, 7))) %>% 
  mutate(day = as.numeric(substring(time2, 9, 10))) %>%
  mutate(daily_avg = (daily_max + daily_min) / 2)

# Bike-share Data
load("data/bikedata.RData")
colnames(starttime) = c("year", "month", "day", "hour", "minute", "second")
df <- data.frame(duration = log(duration), station_start, station_end, 
                starttime, day_of_week, days_since_Jan1_2010, member) %>% 
  # Join with Temperature Data
  left_join(temp_data, by = c('year','month','day')) %>% 
  # Add Weekend/weekday 
  mutate(weekend = day_of_week %in% c("Saturday", "Sunday")) %>% 
  # Add Hour of the Day (with different degree of preciseness)
  mutate(hour_of_day = cut(hour, c(-1, 12, 24))) %>%
  mutate(hour_of_day_2 = cut(hour, c(-1, 6, 12, 18, 24))) %>%
  # Add temperature buckets (with different degree of preciseness)
  mutate(temp = cut(daily_avg, c(20, 40, 60, 80, 100))) %>%
  mutate(temp_2 = cut(daily_avg, c(20, 30, 40, 50, 60, 70, 80, 90, 100))) %>%
  # Assign Id to Route
  group_by(station_start, station_end) %>% 
  mutate(route = cur_group_id()) %>% 
  # Filter out routes with less than 500 records
  filter(length(as.vector(duration)) >= 500) %>% 
  ungroup()

# Remove outliers with IQR range test
outliers <- function(x) {
    q25 <- quantile(x, probs=.25)
    q75 <- quantile(x, probs=.75)
    interval <- q75 - q25
    x > q75 + (interval * 1.5) | x < q25 - (interval * 1.5)
}
# Just remove 5% data on each end
outliers2 <- function(x) {
  x > quantile(x, probs=0.95) | x < quantile(x, probs=0.05)
}

df <- df[!outliers2(df$duration), ]

# Drop unnecessary columns
df <- subset(df, select=-c(station_start, station_end, year, month, day, 
                           minute, second, time1, time2, daily_max, daily_min))
summary(df)
```
```{r}
nrow(df)
df %>%
  group_by(route) %>%
  summarize(count = length(duration), route = route) %>%
  ungroup() %>%
  ggplot(aes(x=count)) +
  geom_histogram()
```
# Permutation test without any control

For each route, we are interested in the question if there is any change in ride duration at any point over the span of 2010-2011. In the language of hypothesis testing, this translates to the hypothesis that ride duration is independent of the days_since_Jan1_2010. Rejecting this hypothesis would mean that ride duration depends on the date - in other words change over time. We observe that the absolute value of correlation between duration and days_since_Jan1_2010 would be relatively small if the hypothesis is true and large otherwise, making it a good candidate statistic for the permutation test. Therefore, we first perform permutation test without any confounder control to gauge how well it works.


``` {r}
permutation.test <- function(duration, days_from_start, n=100) {
  T <- abs(cor(duration, days_from_start))
  distribution <- c()
  for (i in 1:n) {
    distribution[i] <- abs(cor(sample(duration, replace=FALSE), days_from_start))
  }
  p_val <- (1 + sum(distribution >= T)) / (1 + n)
  return (p_val)
}
p_vals <- df %>%
  group_by(route) %>%
  summarize(p_val = permutation.test(duration, days_since_Jan1_2010))

# Adjust p_vals for multiple testing
p_vals$p_adjust = p.adjust(p_vals$p_val, method = 'fdr')
```

## P-value Histogram and Graphs

``` {r}
hist(p_vals$p_adjust, xlab="p_value", main="P-value Histogram", breaks=20)
```

``` {r}
plot_route_change <- function(df, p_vals) {
  sorted_pvals <- p_vals[order(p_vals$p_val), ]
  par(mfrow=c(1,2))
  for (i in 1:2) {
    route_df <- filter(df, route == sorted_pvals$route[i])
    plot(route_df$days_since_Jan1_2010, exp(route_df$duration),
         xlab="Days since Jan 1st 2010", ylab="Duration", type="l")
  }
}
plot_route_change(df, p_vals)
```

# Task-Specific Permutation Tests

However, with confounding variables in the play, a simple permutation scheme will not suffice. For example, it is plausible that members generally ride faster than non-members (who may not be frequent riders), and so the correlation between ride duration and date is confounded by the membership status. To account for these many possible confounders, we use a task-specific permutation testing scheme.

Let $X_j$ be the explanatory variables, and $Y$ be the response, which in our case is the ride duration. Suppose $X_0$ is the days_since_Jan1_2010, what we are actually interested in is the following hypothesis test:
$$ H_0: Y \perp \!\!\! \perp X_0 \; | \; X_{-0} \text{ = all other explanatory variables}$$
However, due to limitations in the dataset, it is infeasible to control for all explanatory variables, as we don't have enough data for each possible variable combinations, and we have not collected every possible ambient variables, such as weather. Therefore, we opted to control a limit set of possible confounders. In particular, we control for the following variables (and some of their combinations):

- membership
- day of the week
- hour of the day
- temperature

During the permutation tests, we only permute data that, for instance, has the same membership status. If we let $X = $ days_from_Jan1_2010, $Z = $ membership status, $Y = $ duration, and $\tilde{X}$ be another days_from_Jan1_2010 data that got permuted, then under the null hypothesis that 
$$ H_0: Y \perp \!\!\! \perp X \; | \; Z $$ 
and an additional assumption that $\mathbb{P}(X \; | \; Z) = \mathbb{P}(\tilde{X} \; | \; Z)$, we have

$$
\begin{aligned}
\mathbb{P}(X,Y,Z) &= \mathbb{P}(X \; | \; Y, Z) \mathbb{P}(Y,Z) \\
&= \mathbb{P}(X \; | \; Z) \mathbb{P}(Y,Z) \qquad \text{Conditional Independence}\\
&= \mathbb{P}(\tilde{X} \; | \; Z) \mathbb{P}(Y,Z) \\
&= \mathbb{P}(\tilde{X}, Y, Z)
\end{aligned}
$$

Therefore, the distribution remains unchanged. This additional assumption that $\mathbb{P}(X \; | \; Z) = \mathbb{P}(\tilde{X} \; | \; Z)$ is reasonable in this case, as we are only saying that a member, or non-member, has the same probability of riding on any day. As such, we adjust the permutation test scheme to be task specific to account for confounders.

Membership status, the day of the week (weekend or weekday), hour of the day, and temperature might be possible confounders. Adjusting the permutation test scheme to also account for these factors can ensure certain original trends in the data, for example the proportion of weekend rides, remain unchanged. We believe these are intuitive confounders as we would expect rides on Monday or Saturday, at 3am or 3pm, in 80F or 50F to be different.

``` {r}
# Task specific Permutation Tests
permutation.task_specific_test <- function(duration, days_from_start, grouping, n=100) {
  T <- abs(cor(duration, days_from_start))
  distribution <- c()
  for (i in 1:n) {
    permuted_duration <- 
      ave(duration, grouping, 
          FUN = function(x) if (length(x) == 1) x else sample(x))
    distribution[i] <- abs(cor(permuted_duration, days_from_start))
  }
  p_val <- (1 + sum(distribution >= T)) / (1 + n)
  return (p_val)
}

# Control for Membership status, day of the week (2), hour of the day(2), and temperature(4)
# paranthesis represents how many categories there are for each variable
group_controlled_pvals <- df %>%
  group_by(route) %>%
  summarize(p_val = permutation.task_specific_test(duration, days_since_Jan1_2010, 
                                                   paste(member, weekend, hour_of_day, temp)))
# Adjust p_vals for multiple testing
group_controlled_pvals$p_adjust = p.adjust(group_controlled_pvals$p_val, method = 'fdr')

# Identify routes with p-val < 0.05
significant_routes <- group_controlled_pvals %>%
  filter(p_adjust < 0.05)

```

## group Controlled P-value Histogram and Graphs

``` {r}
hist(group_controlled_pvals$p_adjust, 
     xlab="p_value", main="P-value Histogram", breaks=20)
```

``` {r}
plot_route_change(df, group_controlled_pvals)
```
We then filter the data to only keep routes with more than 1000 records and apply a finer grouping. Keeping only the routes with more data (>1000), we are able to apply a finer grouping method because there is enough data to ensure the power of the test is still strong even when grouping becomes more specific. Weekdays changed from 2 categories (weekday/weekend) to 7 categories, temperature changed from 20F intervals to 10F intervals, and hour changed from 2 12-hour groups to 4 6-hour groups 

```{r}
# FINER control for Membership status, day of the week (7), hour of the day (6), and temperature (8)
# paranthesis represents how many categories are for each variable

# filter out the routes with more than 1000 data to run the finer control
df_fine <- df %>%
  group_by(route) %>% 
  filter(length(as.vector(duration)) >= 1000) %>% 
  ungroup()

# permutation test with finer controls
group_controlled_pvals2 <- df %>%
  group_by(route) %>%
  summarize(p_val = permutation.task_specific_test(duration, days_since_Jan1_2010, 
                                                   paste(member, day_of_week, hour_of_day_2, temp_2)))
# Adjust p_vals for multiple testing
group_controlled_pvals2$p_adjust = p.adjust(group_controlled_pvals2$p_val, method = 'fdr')

# Identify routes with p-val < 0.05
significant_routes <- group_controlled_pvals2 %>%
  filter(p_adjust < 0.05)

# plot p value histogram
hist(group_controlled_pvals2$p_adjust, 
     xlab="p_value", main="P-value Histogram", breaks=20)
```
``` {r}
plot_route_change(df, group_controlled_pvals)
```

After controlling for the four confounders, we identified routes with p-val < 0.05 after the permutation test. As correlation is more suitable in detecting long-term gradual change in data, we might seek other means to more efficiently identify some sudden changes.



```{r}

```

