---
title: "Regression Approach"
author: "Group members"
date: '2023-02-02'
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(stargazer)
set.seed(123)
```

Initial data cleaning:

```{r, cache=TRUE}
# get the data
# Temperature Data
temp_data <- 
  read.table("data/temperature-data.txt", 
           skip = 1, 
           col.names = c('time1', 'time2', 'daily_max', 'daily_min')) %>% 
  mutate(year = as.numeric(substring(time2, 1, 4))) %>% 
  mutate(month = as.numeric(substring(time2, 6, 7))) %>% 
  mutate(day = as.numeric(substring(time2, 9, 10))) %>%
  mutate(daily_temp = (daily_max + daily_min) / 2)

# Bike-share Data
load("data/bikedata.RData")
colnames(starttime) = c("year", "month", "day", "hour", "minute", "second")
df <- data.frame(log_duration = log(duration), station_start, station_end, 
                starttime, day_of_week, days_since_Jan1_2010, member) %>% 
  # Join with Temperature Data
  left_join(temp_data, by = c('year','month','day')) %>% 
  # Add Weekend/weekday 
  mutate(weekend = day_of_week %in% c("Saturday", "Sunday")) %>% 
  # Add Hour of the Day
  mutate(hour_of_day = cut(hour, c(-1, 6, 12, 18, 24))) %>%
  # Add temperature buckets
  mutate(temp = cut(daily_temp, c(20, 40, 60, 80, 100))) %>%
  # Assign Id to Route
  group_by(station_start, station_end) %>% 
  mutate(route = cur_group_id()) %>% 
  mutate(route = factor(route)) %>% 
  # Filter out routes with less than 500 records
  filter(length(as.vector(log_duration)) >= 500) %>% 
  ungroup()

# Remove outliers with IQR range test
outliers <- function(x) {
    q25 <- quantile(x, probs=.25)
    q75 <- quantile(x, probs=.75)
    interval <- q75 - q25
    x > q75 + (interval * 1.5) | x < q25 - (interval * 1.5)
}
# Just remove 5% data on each end
outliers2 <- function(x) {
  x > quantile(x, probs=0.95) | x < quantile(x, probs=0.05)
}

df <- df[!outliers2(df$log_duration), ]

# drop 2011 Jan-Aug data
df <- df %>% filter(month %in% 9:12) %>% 
  mutate(month = factor(month)) %>% 
  mutate(day_of_week = factor(day_of_week)) %>% 
  mutate(day_of_week = fct_relevel(day_of_week, c("Monday","Tuesday","Wednesday",
                                                  "Thursday","Friday","Saturday","Sunday")))
```


We want to make sure for each route we train a model on, there is enough data.
```{r}
busy_routes_m_df <- df %>% 
  filter(year==2010 & member == T) %>% 
  group_by(route) %>% 
  summarise(ride_counts = n()) %>% 
  filter(ride_counts >= 50)

busy_routes_m <- busy_routes_m_df$route
```


```{r}
# split data by year and membership status
df10_m <- df %>% filter(year == 2010 & member == TRUE & route %in% busy_routes_m) %>% 
  select(c(log_duration, year, month, day, hour, day_of_week, daily_temp, weekend, route))
df10_nm <- df %>% filter(year == 2010 & member == FALSE)

df11_m <- df %>% filter(year == 2011 & member == TRUE & route %in% busy_routes_m)
df11_nm <- df %>% filter(year == 2011 & member == FALSE)
```

Regression formula:

log_duration = month + day_of_week + hour + weekend + daily_temp

## First, work on the membership data.

### Step 1: create a hold-out set for 2010 (for example, 20%)

```{r}
# create hold-out set for 2010
picked = sample(seq_len(nrow(df10_m)), size = nrow(df10_m)*0.8)
df10_m_train =df10_m[picked,]
df10_m_holdout =df10_m[-picked,]
```


### Step 2: train a regression model using 2010’s training data (without aggregation, so that the model covers a wider range for the feature values → less likely for extrapolation to happen)

```{r}
lm_2010 = lm(log_duration ~ month + hour + day_of_week + daily_temp + route, data = df10_m_train)
```

```{r}
summary(lm_2010)
```


## Step 3: Aggregation on 2010 hold-out set

```{r}
df10_m_holdout_agg <- df10_m_holdout %>% 
  select(-c(year, weekend)) %>% 
  group_by(route, month, day, hour) %>% 
  mutate(log_duration = median(log_duration)) %>% 
  ungroup() %>% 
  distinct() %>% 
  arrange(route, month, day, hour)
```


## Step 4: construct prediction intervals for the hold-out data set; then, adjust the width of the prediction intervals so that it captures 95% of the true durations

```{r}
train_routes <- unique(df10_m_train$route)

df10_m_holdout_agg <- df10_m_holdout_agg %>% 
  filter(route %in% train_routes)
```

```{r}
prediction_2010_holdout <- data.frame(predict(lm_2010, newdata = df10_m_holdout_agg, 
                                   interval = "predict", level = 0.95)) %>% 
  mutate(true_duration = df10_m_holdout_agg$log_duration)
```

The original 95% prediction intervals have a non-coverage rate of 0.0488757:

```{r}
nrow(prediction_2010_holdout %>% filter(true_duration>upr | true_duration<lwr)) / nrow(prediction_2010_holdout)
```

Calibrate the prediction intervals so that it has non-coverage rate of 0.05:

```{r}
prediction_2010_holdout_adjusted <- prediction_2010_holdout %>% 
  mutate(upr_adjusted = upr + -0.016 * (upr-fit)) %>% 
  mutate(lwr_adjusted = lwr - -0.016 * (fit-lwr))
```

```{r}
nrow(prediction_2010_holdout_adjusted %>% filter(true_duration>upr_adjusted | true_duration<lwr_adjusted)) / nrow(prediction_2010_holdout_adjusted)
```

That is, we adjust the prediction intervals by making it narrower by a factor of (1-0.016). Now, if the 2011's data doesn't have significant change from 2010's data, using this newly adjusted prediction interval, we should also expect to see a non-coverage rate of roughly 5%.


## Step 5: apply the regression model on the aggregated 2011 data set (Sept. to Dec.); construct prediction intervals and adjust their width; compare this new coverage rate to the one we get from 2010 hold-out set.

Again, we first aggregate 2011's data, just like we did to 2010's hold-out set.

```{r}
df11_m_agg <- df11_m %>% 
  select(-c(year, weekend)) %>% 
  group_by(route, month, day, hour) %>% 
  mutate(log_duration = median(log_duration)) %>% 
  ungroup() %>% 
  distinct() %>% 
  filter(route %in% train_routes) %>% 
  arrange(route, month, day, hour)
```

Then, we create prediction intervals:

```{r}
prediction_2011 <- data.frame(predict(lm_2010, newdata = df11_m_agg, 
                                   interval = "predict", level = 0.95)) %>% 
  mutate(true_duration = df11_m_agg$log_duration)
```

Next, we apply the calibration procedure to adjust the prediction intervals:

```{r}
prediction_2011_adjusted <- prediction_2011 %>% 
  mutate(upr_adjusted = upr + -0.016 * (upr-fit)) %>% 
  mutate(lwr_adjusted = lwr - -0.016 * (fit-lwr))
```

Finally, 

```{r}
nrow(prediction_2011_adjusted %>% filter(true_duration>upr_adjusted | true_duration<lwr_adjusted)) / nrow(prediction_2011_adjusted)
```









